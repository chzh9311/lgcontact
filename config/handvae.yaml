defaults:
  - msdf: sparse
  - data: grab_multi_contact
  - hand_ae: handvae_16latent
  - _self_

training_stage: "handvae"
contact_unit: null

debug: true
data:
  load_msdf: false
  load_grid_contact: false
test_gt: true

train:
  lr: 0.0001
  batch_size: 128
  loss_weights:
    w_param: 0.5
    w_recon: 1.0
    w_kl: 0.000001
  vis_every_n_batches: 200
  augment: false
  resume_ckpt: null
  # pretrained_ckpt: logs/checkpoints/gridae/128latent/best-epoch=22-val/total_loss=0.0970.ckpt

val:
  batch_size: 64
  vis_every_n_batches: 100

test:
  n_processes: 16
  n_samples: 64
  batch_size: ${test.n_samples}

checkpoint:
  dirpath: logs/checkpoints/handvae/${hand_ae.latent_dim}latent/
  filename: best
  save_top_k: 3

trainer: ## For instantiating pytorch-lightning Trainer
  max_epochs: 200
  check_val_every_n_epoch: 2
  log_every_n_steps: 100
  accelerator: gpu
  devices: [0]
  precision: 32
  # enable_progress_bar: true
  enable_model_summary: true
  # strategy: ddp_find_unused_parameters_true
  # limit_train_batches: 100
  # limit_val_batches: 50

run_phase: train
ckpt_path: logs/checkpoints/handvae/64latent-hoi4d/best.ckpt

hydra:
  job:
    chdir: false
  run:
    dir: .
  output_subdir: null
  job_logging:
    disable_existing_loggers: false
  hydra_logging:
    disable_existing_loggers: false