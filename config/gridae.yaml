defaults:
  - msdf: sparse
  - data: grab_local_grid
  - ae: gridae_128latent_v2
  - _self_

training_stage: "ae"

debug: true

train:
  lr: 0.0002
  batch_size: 256
  loss_weights:
    w_contact: 5.0
    w_cse_rec: 0.5
    w_cse_value: 1.0
    w_kl: 0.000001
    w_rec: 0.0
  rec_loss_start_epoch: null
  vis_every_n_batches: 1000
  # resume_ckpt: logs/wandb_logs/LG3DContact/GRIDAE-64v2/epoch=4-wcserec.ckpt
  # pretrained_ckpt: logs/checkpoints/gridae/128latent/best-epoch=22-val/total_loss=0.0970.ckpt
  resume_ckpt: 

val:
  batch_size: 256
  vis_every_n_batches: 500

test:
  n_processes: 16
  n_samples: 512
  batch_size: ${test.n_samples}

checkpoint:
  dirpath: logs/checkpoints/gridae/${ae.feat_dim}latent/
  filename: best
  save_top_k: 3

trainer: ## For instantiating pytorch-lightning Trainer
  max_epochs: 20
  check_val_every_n_epoch: 1
  log_every_n_steps: 100
  accelerator: gpu
  devices: [0]
  precision: 32
  enable_progress_bar: false
  enable_model_summary: true
  # strategy: ddp_find_unused_parameters_true
  # limit_train_batches: 100
  # limit_val_batches: 50

run_phase: train
# ckpt_path: logs/checkpoints/gridae/128latent/best-epoch=16-val/total_loss=0.0948.ckpt
# ckpt_path: # logs/checkpoints/gridae/128latent-v2/best-20epoch.ckpt
ckpt_path: # logs/checkpoints/gridae/mlpres_v2/256latent/best.ckpt

hydra:
  job:
    chdir: false
  run:
    dir: .
  output_subdir: null
  job_logging:
    disable_existing_loggers: false
  hydra_logging:
    disable_existing_loggers: false